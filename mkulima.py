# -*- coding: utf-8 -*-
"""MKULIMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jlQmJmms4jDoC124OV8cZSX96cC5wVMi
"""

! pip install earthengine-api

import ee
# 3️⃣ Authenticate & Initialize GEE
# -------------------------------
ee.Authenticate()
ee.Initialize(project='future-glider-467511-v6')

# -------------------------------

import ee
ee.Initialize(project='future-glider-467511-v6')  # Specify the project ID here

# Load parcel shapefile from your asset
PARCELS_ASSET = 'users/RUKO/gatundu'
parcels = ee.FeatureCollection(PARCELS_ASSET)
print(" Parcels loaded:", parcels.size().getInfo())

!pip install pandas

import ee
import pandas as pd
from datetime import date

PARCELS_ASSET = 'users/RUKO/gatundu'
START_DATE = '2016-01-01'  # <-- define first
END_DATE = date.today().strftime('2025-12-01')  # <-- define second
EXPORT_FOLDER = 'GEE_Exports'
EXPORT_FILE = 'gatundu_features'

#------------------------------
# 3. Load datasets
# -------------------------------
s2 = ee.ImageCollection('COPERNICUS/S2_SR') \
    .filterBounds(parcels) \
    .filterDate(START_DATE, END_DATE)

rain = ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY') \
    .filterBounds(parcels) \
    .filterDate(START_DATE, END_DATE) \
    .select('precipitation')

lst = ee.ImageCollection('MODIS/061/MOD11A1') \
    .filterBounds(parcels) \
    .filterDate(START_DATE, END_DATE) \
    .select('LST_Day_1km') \
    .map(lambda img: img.multiply(0.02).subtract(273.15).rename('LST'))

sm = ee.ImageCollection('NASA_USDA/HSL/SMAP10KM_soil_moisture') \
    .filterBounds(parcels) \
    .filterDate(START_DATE, END_DATE) \
    .select('ssm')

# -------------------------------
# 4. Compute NDVI, EVI, SAVI
# -------------------------------
def add_indices(img):
    ndvi = img.normalizedDifference(['B8','B4']).rename('NDVI')
    evi = img.expression(
        '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))',
        {'NIR': img.select('B8'), 'RED': img.select('B4'), 'BLUE': img.select('B2')}
    ).rename('EVI')
    savi = img.expression(
        '((NIR - RED)/(NIR + RED + 0.5)) * (1 + 0.5)',
        {'NIR': img.select('B8'), 'RED': img.select('B4')}
    ).rename('SAVI')
    return img.addBands([ndvi, evi, savi])

s2_indices = s2.map(add_indices)

# -------------------------------
# 5. Function to compute stats per parcel
# -------------------------------
def compute_stats(img_col, scale):
    def stats_per_parcel(img):
        return img.reduceRegions(
            collection=parcels,
            reducer=ee.Reducer.mean()
                    .combine(ee.Reducer.median(), '', True)
                    .combine(ee.Reducer.stdDev(), '', True),
            scale=scale
        ).map(lambda f: f.set('date', img.date().format('YYYY-MM-dd')))
    return img_col.map(stats_per_parcel).flatten()

ndvi_stats = compute_stats(s2_indices.select('NDVI'), 30)
evi_stats = compute_stats(s2_indices.select('EVI'), 30)
savi_stats = compute_stats(s2_indices.select('SAVI'), 30)
lst_stats = compute_stats(lst, 1000)
sm_stats = compute_stats(sm, 10000)
rain_stats = compute_stats(rain, 5000)

# 6. Compute rolling rainfall
# -------------------------------
def rolling_rainfall(rain_col, days):
    n_days = ee.Date(END_DATE).difference(START_DATE, 'day')
    dates = ee.List.sequence(0, n_days)

    def sum_window(date_int):
        date = ee.Date(START_DATE).advance(date_int, 'day')
        window_start = date.advance(-days, 'day')
        return rain_col.filterDate(window_start, date).sum().rename(f'Rain_{days}d').set('date', date.format('YYYY-MM-dd'))

    return ee.ImageCollection(dates.map(sum_window))

rain7 = rolling_rainfall(rain, 7)
rain30 = rolling_rainfall(rain, 30)
rain60 = rolling_rainfall(rain, 60)

#-------------------------------
# 7. Merge stats (sample preview)
# -------------------------------
# Here, we show NDVI + EVI merged for the first 5 features
ndvi_sample = ndvi_stats.limit(5).getInfo()['features']
evi_sample = evi_stats.limit(5).getInfo()['features']

merged_sample = []
for ndvi_f, evi_f in zip(ndvi_sample, evi_sample):
    props = {**ndvi_f['properties'], **evi_f['properties']}
    merged_sample.append(props)

df_preview = pd.DataFrame(merged_sample)
print(" Preview of merged NDVI + EVI stats (first 5 features):")
print(df_preview.head())

# -------------------------------
# 9. Export NDVI stats as CSV (merge full later)
# -------------------------------
task = ee.batch.Export.table.toDrive(
    collection=ndvi_stats,  # later merge all features into one CSV if needed
    description='Gatundu_All_Features_Export',
    folder=EXPORT_FOLDER,
    fileNamePrefix=EXPORT_FILE,
    fileFormat='CSV'
)
task.start()
print(" Export task started. Check Google Drive folder:", EXPORT_FOLDER)

# 10. Export LST stats as CSV (merge full later)
# -------------------------------
task = ee.batch.Export.table.toDrive(
    collection=lst_stats,  # later merge all features into one CSV if needed
    description='LST_Features_Export',
    folder=EXPORT_FOLDER,
    fileNamePrefix=EXPORT_FILE,
    fileFormat='CSV'
)

task.start()
print(" Export task started. Check Google Drive folder:", EXPORT_FOLDER)

# 10. Export evi_stats stats as CSV (merge full later)
# -------------------------------
task = ee.batch.Export.table.toDrive(
    collection=evi_stats,  # later merge all features into one CSV if needed
    description='EVI_Features_Export',
    folder=EXPORT_FOLDER,
    fileNamePrefix=EXPORT_FILE,
    fileFormat='CSV'
)

task.start()
print(" Export task started. Check Google Drive folder:", EXPORT_FOLDER)

# 10. Export sm_stats stats as CSV (merge full later)
# -------------------------------
task = ee.batch.Export.table.toDrive(
    collection=sm_stats,  # later merge all features into one CSV if needed
    description='EVI_Features_Export',
    folder=EXPORT_FOLDER,
    fileNamePrefix=EXPORT_FILE,
    fileFormat='CSV'
)

task.start()
print(" Export task started. Check Google Drive folder:", EXPORT_FOLDER)

# 10. Export rain_stats stats as CSV (merge full later)
# -------------------------------
task = ee.batch.Export.table.toDrive(
    collection=rain_stats,  # later merge all features into one CSV if needed
    description='EVI_Features_Export',
    folder=EXPORT_FOLDER,
    fileNamePrefix=EXPORT_FILE,
    fileFormat='CSV'
)

task.start()
print(" Export task started. Check Google Drive folder:", EXPORT_FOLDER)

## WE WORK ON CROP STRESS INDEX USING Random Forest

from google.colab import drive
import os
import pandas as pd

# 1. Mount Google Drive
drive.mount('/content/drive')

# 2. Define folder path — adjust if your folder is in a sub‑folder
folder_path = '/content/drive/MyDrive/INDICES_CSV'
# 3. List CSV files to confirm access
csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
print("Found CSV files:", csv_files)

# 4. Load and merge
dfs = [pd.read_csv(os.path.join(folder_path, f)) for f in csv_files]
data = pd.concat(dfs, ignore_index=True)

print(data.head())
print(data.info())

# 2. Load and merge CSVs
# ===============================
import pandas as pd
import os

print(data.columns.tolist())

import os

folder_path = '/content/drive/MyDrive/INDICES_CSV'
csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
print("Found CSV files:", csv_files)

columns_to_keep = ['OBJECTID', 'date', 'mean']  # adjust per CSV

# Initialize merged_data with the first CSV file
first_file = csv_files[0]
merged_data = pd.read_csv(os.path.join(folder_path, first_file))
merged_data = merged_data[columns_to_keep]
new_col_name_first = f"mean_{os.path.splitext(first_file)[0]}"
merged_data = merged_data.rename(columns={'mean': new_col_name_first})

# Merge remaining CSVs
for f in csv_files[1:]:
    df = pd.read_csv(os.path.join(folder_path, f))
    # Keep only OBJECTID, date, and mean
    df = df[columns_to_keep]

    # Rename 'mean' column to avoid duplicates (use filename as suffix)
    new_col_name = f"mean_{os.path.splitext(f)[0]}"
    df = df.rename(columns={'mean': new_col_name})

    # Merge
    merged_data = pd.merge(merged_data, df, on=['OBJECTID', 'date'], how='outer')

# Inspect merged data
print(merged_data.head())
print(merged_data.columns.tolist())

# Keep only identifiers and feature columns
feature_cols = [col for col in merged_data.columns if col.startswith('mean_')]
ml_data = merged_data[['OBJECTID', 'date'] + feature_cols].copy()

# Inspect
print(ml_data.head())
print(ml_data.columns.tolist())

# Keep only identifiers and feature columns
ml_data = merged_data[['OBJECTID', 'date', 'mean_LST', 'mean_EVI', 'mean_SM', 'mean_RAINFALL']].copy()

# Drop rows with missing feature values (optional)
ml_data = ml_data.dropna(subset=['mean_LST','mean_EVI','mean_SM','mean_RAINFALL'])

# Inspect
print(ml_data.head())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Example: weighted CSI using EVI, SM, and LST
evi_max = ml_data['mean_EVI'].max()
evi_min = ml_data['mean_EVI'].min()

ml_data['CSI_ref'] = (
    0.5 * (evi_max - ml_data['mean_EVI']) / (evi_max - evi_min) +
    0.3 * (ml_data['mean_SM'].max() - ml_data['mean_SM']) / (ml_data['mean_SM'].max() - ml_data['mean_SM'].min()) +
    0.2 * (ml_data['mean_LST'] - ml_data['mean_LST'].min()) / (ml_data['mean_LST'].max() - ml_data['mean_LST'].min())
)

## Inspect top rows of features and new multi-factor CSI
print(ml_data[['mean_EVI', 'mean_SM', 'mean_LST', 'mean_RAINFALL', 'CSI_ref']].head(10))

# Compute multi-index reference CSI
# Normalize each feature between 0 and 1
evi_norm = (ml_data['mean_EVI'].max() - ml_data['mean_EVI']) / (ml_data['mean_EVI'].max() - ml_data['mean_EVI'].min())
sm_norm = (ml_data['mean_SM'].max() - ml_data['mean_SM']) / (ml_data['mean_SM'].max() - ml_data['mean_SM'].min())
lst_norm = (ml_data['mean_LST'] - ml_data['mean_LST'].min()) / (ml_data['mean_LST'].max() - ml_data['mean_LST'].min())
rain_norm = (ml_data['mean_RAINFALL'].max() - ml_data['mean_RAINFALL']) / (ml_data['mean_RAINFALL'].max() - ml_data['mean_RAINFALL'].min())

# Weighted combination to get multi-factor CSI
ml_data['CSI_ref'] = 0.4*evi_norm + 0.3*sm_norm + 0.2*lst_norm + 0.1*rain_norm

# Inspect top rows
print(ml_data[['mean_EVI','mean_SM','mean_LST','mean_RAINFALL','CSI_ref']].head(10))

# Features
X = ml_data[['mean_LST','mean_EVI','mean_SM','mean_RAINFALL']]
y = ml_data['CSI_ref']

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train RF
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)

# Predict
y_pred = rf.predict(X_test)

# Evaluate
print("R2 Score:", r2_score(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))

# Feature importance
import matplotlib.pyplot as plt
plt.barh(X.columns, rf.feature_importances_, color='green')
plt.xlabel('Importance')
plt.title('Feature Importance for Sweet Potato Stress Prediction')
plt.show()

# Add predictions to DataFrame
ml_data['Predicted_CSI'] = rf.predict(X)

def insurance_risk(csi):
    if csi < 0.3:
        return 'Low Risk'         # Little/no crop stress → low payout
    elif csi < 0.6:
        return 'Medium Risk'      # Moderate stress → partial payout
    else:
        return 'High Risk'        # Severe stress → full payout

ml_data['Insurance_Risk'] = ml_data['Predicted_CSI'].apply(insurance_risk)

# Inspect
print(ml_data[['date','OBJECTID','Predicted_CSI','Insurance_Risk']].head(10))

ml_data['Predicted_CSI']
ml_data['Insurance_Risk']

import joblib

# Save trained model
joblib.dump(rf, "sweet_popatoes_stress_model.pkl")

# Save scaler if you used one (optional)
# joblib.dump(scaler, "scaler.pkl")

from google.colab import drive
drive.mount('/content/drive')

!cp sweet_popatoes_stress_model.pkl /content/drive/MyDrive/

